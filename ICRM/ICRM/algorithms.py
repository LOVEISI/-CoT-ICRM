# Copyright (c) Meta Platforms, Inc. and affiliates.
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.

from __future__ import division
from __future__ import print_function
import torch
import utils
import copy
import networks
import numpy as np
import torch.nn.functional as F
import torch.autograd as autograd
import os

ALGORITHMS = ['ERM', 'ICRM', 'ARM_CML', 'TENT', 'Mixup', 'Fish', 'IB_ERM', 'IB_IRM']

def get_algorithm_class(algorithm_name):
    """Return the algorithm class with the given name."""
    if algorithm_name not in globals():
        raise NotImplementedError("Algorithm not found: {}".format(algorithm_name))
    return globals()[algorithm_name]

class Algorithm(torch.nn.Module):
    """
    A subclass of Algorithm implements a domain generalization algorithm.
    Subclasses should implement the following:
    - update()
    - predict()
    """
    def __init__(self, input_shape, num_classes, hparams):
        super(Algorithm, self).__init__()
        self.num_classes = num_classes
        self.hparams = hparams
        self.metrics = hparams['metrics']
        self.loss_func = utils.get_loss_function(hparams['loss'])
        self.device = hparams['device']
        self.n_tasks_per_step = hparams['n_sampled_tasks'] if hparams.get('n_sampled_tasks') else 0
        
        self.featurizer = networks.Featurizer(input_shape, self.hparams)
        self.classifier = networks.Classifier(self.featurizer.n_outputs, num_classes, self.hparams)
        
        if hparams['is_parallel']:
            print('=> Using data parallel')
            self.featurizer = utils.data_parallel(self.featurizer)
            self.classifier = utils.data_parallel(self.classifier)
                
        self.network = torch.nn.Sequential(self.featurizer, self.classifier)
        self.optimizer = utils.extract_optimizer(self.hparams['optimizer_name'], self.network.parameters(), lr = self.hparams['lr'], weight_decay = self.hparams['weight_decay'])
        
        
    def update(self, minibatches, unlabeled=None):
        """
        Perform one update step, given a list of (x, y) tuples for all
        environments.

        Admits an optional list of unlabeled minibatches from the test domains,
        when task is domain_adaptation.
        """
        raise NotImplementedError

    def predict(self, x, y = None, model = None):
        raise NotImplementedError
    
    def evaluate(self, loader, weights = None, metrics = ['accuracy']):
        raise NotImplementedError()
    
from torch.cuda.amp import autocast, GradScaler
class ERM(Algorithm):
    """
    Empirical Risk Minimization (ERM)
    """
    def __init__(self, input_shape, num_classes, hparams):
        super(ERM, self).__init__(input_shape, num_classes,
                                  hparams)
        self.scaler = GradScaler()

    
    # def update(self, minibatches, unlabeled=None):
    #     self.network.train()
    #     # self.train() 
    #     if self.n_tasks_per_step != 0: #æ­¥è®­ç»ƒæ—¶åªé€‰æ‹©ä¸€éƒ¨åˆ†ä»»åŠ¡è¿›è¡Œè®­ç»ƒ
    #         indices = torch.randperm(len(minibatches))[:min(self.n_tasks_per_step, len(minibatches))]
    #         minibatches = [minibatches[i] for i in indices] #æ ¹æ®éšæœºé€‰æ‹©çš„ç´¢å¼•ï¼Œé€‰æ‹©æ‰¹æ¬¡æ•°æ®ã€‚



    #     all_y, p = [], [] #åˆ†åˆ«ç”¨äºå­˜å‚¨æ‰€æœ‰æ‰¹æ¬¡çš„çœŸå®æ ‡ç­¾ï¼ˆyï¼‰å’Œæ¨¡å‹çš„é¢„æµ‹è¾“å‡ºï¼ˆpï¼‰ã€‚
    #       # 1) æ¸…é›¶æ¢¯åº¦
    #     self.optimizer.zero_grad()

    #     # 2) AMP å‰å‘è¿‡ç¨‹
    #     with autocast():
    #         for _, (x,y) in enumerate(minibatches):
    #             utils.d_print('Train input shape is', x.shape, y.shape)
    #             # print("å¼€å§‹è®­ç»ƒäº†å“ˆ")
    #             out = self.predict(x, y) #æ‰§è¡Œå‰å‘ä¼ æ’­
    #             utils.d_print('Output shape is', out.shape)
    #             p.append(out); all_y.append(y)
        
    #         p = torch.cat(p, dim=0)
    #         all_y = torch.cat(all_y, dim=0).view(-1).long()        
    #         loss = self.loss_func(p, all_y)
        
    #     # Compute performance metrics
    #     metric_results = utils.compute_metric(self.metrics, p, all_y)#è®¡ç®—æ€§èƒ½æŒ‡æ ‡
    #     metric_results = {'train_' + key:val for key, val in metric_results.items()}
    #     metric_results['train_loss'] = loss.item()

    #     # # Model parameters update
    #     # self.optimizer.zero_grad()
    #     # loss.backward()
    #     # self.optimizer.step()
    #       # 4) AMP åå‘+ä¼˜åŒ–
    #     # scale the loss, call backward, then unscale & step
    #     self.scaler.scale(loss).backward()
    #     self.scaler.step(self.optimizer)
    #     self.scaler.update()
    #     return metric_results
    def update(self, minibatches, unlabeled=None):
        # å°†æ•´ä¸ªç½‘ç»œåˆ‡åˆ°è®­ç»ƒæ¨¡å¼ï¼ˆå¯ç”¨ Dropout/BN çš„è®­ç»ƒè¡Œä¸ºï¼‰
        self.network.train()

        # ç»Ÿè®¡ç”¨ï¼šç´¯è®¡æŸå¤±ä¸æ ·æœ¬æ•°ï¼ˆç”¨äºepochå‡å€¼ï¼‰ï¼›æ—¥å¿—æŒ‡æ ‡ç¼“å†²åŒº
        total_loss, total_n = 0.0, 0
        all_logits, all_targets = [], []  # ä»…ç”¨äºç»Ÿè®¡è®­ç»ƒæŒ‡æ ‡ï¼Œä¸å‚ä¸æ¢¯åº¦

        # è¯»å– GPT-2 çš„ä½ç½®ä¸Šé™ï¼ˆåºåˆ—é•¿åº¦ï¼‰ï¼Œæ²¡æœ‰åˆ™é»˜è®¤ 1024
        n_pos  = getattr(self.classifier._backbone.config, 'n_positions', 1024)
        # çº¦å®š label_embed çš„æœ€åä¸€ä¸ª id ä½œä¸º BOSï¼ˆåºåˆ—èµ·å§‹æ ‡ç­¾ï¼‰
        bos_id = self.classifier.label_embed.num_embeddings - 1  # çº¦å®š BOS åœ¨æœ€åä¸€ä¸ª id

        # éå†ä¸€ä¸ª epoch å†…çš„æ‰€æœ‰ minibatch
        for _, (x, y) in enumerate(minibatches):
            # é€å…¥è®¾å¤‡ï¼›x å¯èƒ½æ˜¯ [B,3,H,W] æˆ– [B,T,3,H,W]ï¼›y å¯èƒ½æ˜¯ [B] æˆ– [B,T]
            x = x.to(self.device)                    # å›¾åƒå¼ é‡
            y = y.to(self.device).long()             # æ ‡ç­¾å¼ é‡ï¼ˆæ•´å‹ï¼‰

            # è‹¥åªæœ‰å•æ­¥ï¼Œåˆ™è¡¥æ—¶é—´ç»´åˆ° T=1ï¼Œç»Ÿä¸€æˆ [B,T,3,H,W] / [B,T]
            if x.ndim == 4: x = x.unsqueeze(1)       # -> [B,1,3,H,W]
            if y.ndim == 1: y = y.unsqueeze(1)       # -> [B,1]
            B, T = x.size(0), x.size(1)              # æ‰¹å¤§å°ä¸æ—¶é—´æ­¥æ•°

            # æ¯ä¸€æ­¥ä¼šäº§ç”Ÿ 2 ä¸ª tokenï¼ˆx_token ä¸ y_tokenï¼‰ï¼Œæ€»æ­¥æ•°å¤ªå¤§ä¼šè¶… n_positions
            # ç®€å•ç­–ç•¥ï¼šæŒ‰çª—å£è®­ç»ƒï¼Œæ¯ä¸ªçª—å£é•¿åº¦ä¸è¶…è¿‡ n_pos//2ï¼›çª—å£ä¹‹é—´ä¸ä¼ é€’ past
            max_steps = max(1, n_pos // 2)

            # æ¸…é›¶è¯¥ batch çš„æ¢¯åº¦
            self.optimizer.zero_grad()

            # è‡ªåŠ¨æ··åˆç²¾åº¦ä¸Šä¸‹æ–‡ï¼ˆAMPï¼‰ï¼Œå‡å°‘æ˜¾å­˜/åŠ é€Ÿï¼›éœ€æ­é… GradScaler æ›´ç¨³ï¼ˆæ­¤å¤„æŒ‰ä½ åŸé€»è¾‘ï¼‰
            with autocast():
                # è·¨æ—¶é—´æ­¥ç´¯ç§¯çš„æŸå¤±ï¼ˆæ ‡é‡å¼ é‡ï¼‰ï¼›ä»¥åŠæ—¥å¿—ç¼“å†²ï¼ˆé€æ­¥æ”¶é›† [B,C] å’Œ [B]ï¼‰
                loss_accum = None
                step_logits_buf = []   # ä¿å­˜æ¯ä¸ªæ—¶é—´æ­¥çš„ logitsï¼ˆå½¢çŠ¶ [B,C]ï¼‰
                step_targets_buf = []  # ä¿å­˜æ¯ä¸ªæ—¶é—´æ­¥çš„ç›®æ ‡ y_tï¼ˆå½¢çŠ¶ [B]ï¼‰

                # ä»¥çª—å£ä¸ºå•ä½éå†æ•´æ®µåºåˆ—ï¼ˆé¿å…è¶…è¿‡ n_positionsï¼‰
                start = 0
                while start < T:
                    end  = min(T, start + max_steps) # å½“å‰çª—å£å³è¾¹ç•Œï¼ˆä¸å«ï¼‰
                    past = None                      # çª—å£èµ·ç‚¹ä¸ç»§æ‰¿ä¸Šçª—å£çš„ KV cache
                    # çª—å£çš„ç¬¬ä¸€æ­¥ä½¿ç”¨ BOS ä½œä¸ºâ€œä¸Šä¸€æ—¶åˆ»æ ‡ç­¾â€ï¼›å½¢çŠ¶ [B,1]
                    prev_y = torch.full((B, 1), bos_id, dtype=torch.long, device=self.device)

                    # åœ¨çª—å£å†…åšé€æ­¥è‡ªå›å½’ï¼št ä» start åˆ° end-1
                    for t in range(start, end):
                        # å–ç¬¬ t ä¸ªæ—¶é—´æ­¥çš„å›¾åƒï¼Œä¿æŒæ—¶é—´ç»´é•¿åº¦ä¸º 1ï¼šå½¢çŠ¶ [B,1,3,H,W]
                        x_step = x[:, t:t+1]  # åˆ‡ç‰‡ä¸å¤åˆ¶æ•°æ®

                        # ä¸€æ­¥è‡ªå›å½’ï¼šæŠŠ (x_t, prev_y) äº¤ç»™æ¨¡å‹ï¼ŒåŒæ—¶ä¼ å…¥ pastï¼ˆå†å² KVï¼‰
                        # è¿”å›ï¼š
                        #   logits_1: æœ¬æ­¥é¢„æµ‹çš„åˆ†ç±» logitsï¼Œå½¢çŠ¶ [B,1,C]ï¼ˆåªå¯¹åº”æœ¬æ­¥çš„ y_tï¼‰
                        #   past    : ç´¯ç§¯åçš„ KVï¼Œç”¨äºä¸‹ä¸€æ­¥ç»§ç»­è§£ç 
                        logits_1, past = self.predict(
                            x_step, prev_y, return_context=True, past_key_values=past
                        )

                        # å–æœ¬æ­¥çš„ç›‘ç£ä¿¡å· y_tï¼ˆçœŸå€¼ï¼‰ï¼Œå½¢çŠ¶ [B]
                        target_t = y[:, t]

                        # ç¼“å­˜åˆ°æ—¥å¿—ç¼“å†²åŒºï¼ˆdetachï¼Œé¿å…å‚ä¸åä¼ ï¼‰
                        step_logits_buf.append(logits_1[:, 0, :])  # [B,C]
                        step_targets_buf.append(target_t)          # [B]

                        # è®¡ç®—æœ¬æ­¥çš„äº¤å‰ç†µæŸå¤±ï¼›é»˜è®¤ reduction='mean'ï¼ˆæŒ‰ batch æ±‚å‡å€¼ï¼‰
                        loss_t = self.loss_func(logits_1[:, 0, :], target_t)

                        # å°†æ¯æ­¥æŸå¤±ç›¸åŠ ï¼Œå¾—åˆ°æ•´ä¸ªçª—å£ï¼ˆæˆ–æ•´ä¸ªåºåˆ—ï¼‰çš„æ€»æŸå¤±
                        if loss_accum is None:
                            loss_accum = loss_t
                        else:
                            loss_accum = loss_accum + loss_t

                        # çº¯è‡ªå›å½’ï¼šä¸‹ä¸€æ­¥çš„â€œä¸Šä¸€æ ‡ç­¾â€ç”¨å½“å‰æ­¥çš„æ¨¡å‹é¢„æµ‹ï¼ˆargmaxï¼‰
                        # ç”¨ detach() é˜»æ–­æ¢¯åº¦ï¼›keepdim=True ä¿æŒå½¢çŠ¶ [B,1]
                        prev_y = logits_1[:, 0, :].detach().argmax(dim=-1, keepdim=True)

                    # ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ªçª—å£
                    start = end

            # åå‘ä¼ æ’­ç´¯è®¡çš„æŸå¤±ï¼ˆè·¨æ—¶é—´æ­¥ä¹‹å’Œï¼‰ï¼Œå¹¶æ›´æ–°å‚æ•°
            loss_accum.backward()
            self.optimizer.step()

            # ç»Ÿè®¡ï¼šå°†è¯¥ batch çš„æŸå¤±ï¼ˆä¹˜ä»¥æ ·æœ¬æ•° Bï¼‰ç´¯åŠ ï¼Œç¨åç”¨äºæ±‚å¹³å‡æŸå¤±
            total_loss += float(loss_accum.item()) * B
            total_n    += B

            # ä»…ç”¨äºæ—¥å¿—æŒ‡æ ‡ç»Ÿè®¡ï¼ˆä¸å‚ä¸æ¢¯åº¦ï¼Œå·²åœ¨ä¸Šé¢ append æ—¶ detachï¼‰
            # å°†æœ¬ batch çš„æ‰€æœ‰æ—¶é—´æ­¥æ‹¼æ¥ï¼šlogits -> [B*T, C]ï¼›targets -> [B*T]
            all_logits.append(torch.cat(step_logits_buf, dim=0).detach())
            all_targets.append(torch.cat(step_targets_buf, dim=0).detach())

        # æ±‡æ€»æ•´ä¸ª epoch çš„è®­ç»ƒæŒ‡æ ‡
        p     = torch.cat(all_logits, dim=0)   # [Î£(B*T), C]
        all_y = torch.cat(all_targets, dim=0)  # [Î£(B*T)]
        metric = utils.compute_metric(self.metrics, p, all_y)      # ä¾‹å¦‚ {'accuracy': ...}
        metric = {f"train_{k}": v for k, v in metric.items()}      # æ‰“ä¸Š 'train_' å‰ç¼€
        metric["train_loss"] = total_loss / max(total_n, 1)        # å¹³å‡è®­ç»ƒæŸå¤±
        return metric                                              # è¿”å›æ—¥å¿—æŒ‡æ ‡å­—å…¸


    def predict(self, x, y = None, model = None):
        print("ä¸å–œå–œ")
        return (self.network if model is None else model)(x) 

    def _evaluate(self, model, loader, metrics=['accuracy']):
        metrics = metrics or self.metrics
        model.eval()
        result = {key: 0 for key in metrics}
        total = 0
        with torch.no_grad():
            for x, y in loader:
                x, y = x.to(self.device), y.to(self.device)
                p = self.predict(x, y, model=model)
                batch_results = utils.compute_metric(metrics, p, y)
                for metric in metrics:
                    result[metric] += batch_results[metric] * len(y)
                total += len(y)
        for metric in metrics:  result[metric] /= (total + 1e-9)
        model.train()
        return result

    def evaluate(self, loader, n_test_samples = 100, module = 'train', cache = None):
        self.network.eval()
        result = {}
        metric_results = self._evaluate(self.network, loader, self.hparams['metrics'])           
        self.test_ctxt = range(0, 51, 5) if module == 'test' else [0, 25, 50, 75, 100]
        for num_samples in self.test_ctxt:
            result.update({f'{metric}(e-{n_test_samples - num_samples})': metric_results[metric] for metric in self.hparams['metrics']})
        self.network.train()
        return result
    
    def _get_ckpt_metric(self):
        return f'acc(e-0)'
 

class TENT(ERM):
    """Tent: Fully Test-Time Adaptation by Entropy Minimization"""
    def __init__(self, input_shape, num_classes, hparams):
        # Make sure to load weights of a trained ERM model before fine-tuning it with TENT
        super().__init__(input_shape, num_classes, hparams)
        self.n_steps = hparams.get('n_steps', 10)
        self.episodic = hparams.get('episodic', 1)
        print(f'Using episodic {bool(self.episodic)} training with {self.n_steps} steps')
        self.flag = 0
    
    def init_states(self):
        self.model_state, self.optimizer_state = self.copy_model_and_optimizer(self.network, self.optimizer)
        self.flag = 1
                
    def _setup_model(self):
        if not self.flag:
            self.o_network = copy.deepcopy(self.network)
            self.o_network.eval()
        self.network = self._configure_model(self.network)
        params, _ = self._collect_params(self.network)
        self.optimizer = utils.extract_optimizer(self.hparams['optimizer_name'], params, lr = self.hparams['lr'], weight_decay = self.hparams['weight_decay'])
    
    def evaluate(self, loader, module = 'train', cache = None):
        if not self.flag:
            self._setup_model()
            self.init_states()
        
        self.reset()
        self._setup_model()

        self.test_ctxt = list(range(0, 51, 5)) if module == 'test' else [25, 50, 75, 100]

        if 0 in self.test_ctxt:
            metric_results = self._evaluate(self.o_network, loader, self.hparams['metrics'])  
            result = {f'{metric}(e-100)': metric_results[metric] for metric in self.hparams['metrics']}
        else:
            result = {}
         
        assert self.n_steps > 0, "Tent requires >= 1 step(s) to forward and update"
        for n_samples in self.test_ctxt:
            if n_samples == 0:
                continue
            self.reset()
            metric_results = {key: 0 for key in self.hparams['metrics']}
            sub_loader = torch.utils.data.DataLoader(loader.dataset, batch_size=n_samples, shuffle=False)
            total = 0
            for _, (x, y) in enumerate(sub_loader):
                x, y = x.to(self.device), y.to(self.device)
                if self.episodic:   
                    self.reset()
                t_loss = []
                for _ in range(self.n_steps):
                    loss, p = self._forward_and_adapt(x, self.network, self.optimizer)    
                    t_loss.append(loss.item())
                batch_results = utils.compute_metric(self.hparams['metrics'], p, y)
                for metric in self.hparams['metrics']:
                    metric_results[metric] += batch_results[metric] * len(y)
                total += len(y)
            for metric in self.hparams['metrics']:  metric_results[metric] /= (total + 1e-9)  
            result.update({f'{metric}(e-{100 - n_samples})': metric_results[metric] for metric in self.hparams['metrics']})       
        return result
    
    def reset(self):
        if self.model_state is None or self.optimizer_state is None:
            raise Exception("cannot reset without saved model/optimizer state")
        self.load_model_and_optimizer(self.network, self.optimizer, self.model_state, self.optimizer_state)

    @torch.enable_grad()                                                                    # ensure grads in possible no grad context for testing
    def _forward_and_adapt(self, x, model, optimizer):
        """Forward and adapt model on batch of data.
        Measure entropy of the model prediction, take gradients, and update params.
        """
        outputs = model(x)
        loss = utils.softmax_entropy(outputs).mean(0)
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
        return loss, outputs
       
    @staticmethod
    def copy_model_and_optimizer(model, optimizer):
        """Copy the model and optimizer states for resetting after adaptation."""
        model_state = copy.deepcopy(model.state_dict())
        optimizer_state = copy.deepcopy(optimizer.state_dict())
        return model_state, optimizer_state

    @staticmethod
    def load_model_and_optimizer(model, optimizer, model_state, optimizer_state):
        """Restore the model and optimizer states from copies."""
        model.load_state_dict(model_state, strict=True)
        optimizer.load_state_dict(optimizer_state)
        
    @staticmethod
    def _configure_model(model):
        """Configure model for use with tent."""
        model.train()                                                                       # train mode, because tent optimizes the model to minimize entropy
        model.requires_grad_(False)                                                         # disable grad, to (re-)enable only what tent updates
        for m in model.modules():                                                           # configure norm for tent updates: enable grad + force batch statisics
            if isinstance(m, torch.nn.BatchNorm2d):
                m.requires_grad_(True)
                m.track_running_stats = False                                               # force use of batch stats in train and eval modes
                m.running_mean = None
                m.running_var = None
        return model 
    
    @staticmethod
    def _collect_params(model):
        """Collect the affine scale + shift parameters from batch norms.
        Walk the model's modules and collect all batch normalization parameters.
        Return the parameters and their names.
        Note: other choices of parameterization are possible!
        """
        params, names = [], []
        for nm, m in model.named_modules():
            if isinstance(m, torch.nn.BatchNorm2d):
                for np, p in m.named_parameters():
                    if np in ['weight', 'bias']:                                           # weight is scale, bias is shift
                        params.append(p)
                        names.append(f"{nm}.{np}")
        return params, names
    
    @staticmethod
    def _check_model(model):
        """Check model for compatability with tent."""
        is_training = model.training
        assert is_training, "tent needs train mode: call model.train()"
        param_grads = [p.requires_grad for p in model.parameters()]
        has_any_params = any(param_grads)
        has_all_params = all(param_grads)
        assert has_any_params, "tent needs params to update: check which require grad"
        assert not has_all_params, "tent should not update all params: check which require grad"
        has_bn = any([isinstance(m, torch.nn.BatchNorm2d) for m in model.modules()])
        assert has_bn, "tent needs normalization for its optimization"  
        
        

class ICRM(ERM):
    """
    In Context Learner (ICRM)
    """
    def __init__(self, input_shape, num_classes, hparams):
        self.context_len = hparams['context_length']
        self.train_context_len = 100
        hparams['is_transformer'] = 1
        super(ICRM, self).__init__(input_shape, num_classes,
                                  hparams)
                
        # æ‰“å° Featurizer ç»“æ„
        # print("Featurizer structure:")
        # print(self.featurizer)
         # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹çš„æ­¥éª¤
        # pretrained_model_path = hparams['pretrained_model_path']  # ä½ å¯ä»¥åœ¨ hparams ä¸­è®¾ç½®é¢„è®­ç»ƒæ¨¡å‹çš„è·¯å¾„
        # if os.path.exists(pretrained_model_path):
        #     print(f"Loading pretrained model from {pretrained_model_path}...")
        #     pretrained_state_dict = torch.load(pretrained_model_path, map_location=self.device)
        #     # æ‰“å°æ¨¡å‹çš„ state_dict é”®å€¼ï¼Œä»¥æŸ¥çœ‹æ¯ä¸ªå±‚çš„åç§°
        #     # for name, param in pretrained_state_dict.items():
        #     #     print(f"Layer: {name}, Shape: {param.shape}")
            
        #     # 1) ç»™æ‰€æœ‰ key åŠ  module. å‰ç¼€ï¼ˆå¦‚æœä¹‹å‰è¿˜æ²¡åšï¼‰
        #     new_sd = {}
        #     for k, v in pretrained_state_dict.items():
        #         new_sd[ k] = v

        #     # 2) æ‹¿å‡ºç¬¬ 1 å±‚ conv çš„æƒé‡ï¼Œæ‰©æˆ 3 é€šé“
        #     w1 = new_sd['context_net.0.weight']  # [128,1,5,5]
        #     # æ–¹æ³• 2.1ï¼šç›´æ¥å¤åˆ¶ä¸‰ä»½
        #     w1_3c = w1.repeat(1, 3, 1, 1)               # [128,3,5,5]
        #     # ï¼ˆå¯é€‰ï¼‰æ–¹æ³• 2.2ï¼šå¤åˆ¶åé™¤ä»¥ 3ï¼Œä¿æŒæ•°å€¼å¹…åº¦ä¸€è‡´
        #     # w1_3c = w1.repeat(1, 3, 1, 1) / 3

        #     new_sd['context_net.0.weight'] = w1_3c
        #     # new_sd['context_net.0.weight'] = w1
        #     # ç»™æ¯ä¸ª key å‰é¢åŠ ä¸Š module.
        #     new_sd = {k: v for k, v in new_sd.items() if k not in ["projector.weight", "projector.bias"]}
        #     # å‡è®¾é¢„è®­ç»ƒæ¨¡å‹åªæœ‰ featurizer éƒ¨åˆ†
        #     self.featurizer.load_state_dict(new_sd, strict=True)  # åŠ è½½é¢„è®­ç»ƒæƒé‡ï¼Œstrict=False å…è®¸éƒ¨åˆ†åŠ è½½
        #     print("Pretrained model loaded successfully!")
        # else:
        #     print(f"No pretrained model found at {pretrained_model_path}.")
        # ä»¥ä¸Šæ˜¯colouredMNISTçš„featç‰ˆæœ¬
        
        
        # pretrained_path = hparams['pretrained_model_path']
        # raw_sd = torch.load(pretrained_path, map_location=self.device)
        # # print("raw_sd å…±æœ‰", len(raw_sd), "ä¸ªæ¡ç›®ï¼š")
        # # for k in raw_sd.keys():
        # #     print("-", k)

        # model_sd = self.featurizer.state_dict()
        # model_keys = set(model_sd.keys())
        # fixed_sd = {}
        # prefix = "module.network.features."

        # for k, v in raw_sd.items():
        #     # 1) å»æ‰å¼€å¤´çš„ "enc."
        #     if k.startswith("enc."):
        #         stripped = k[len("enc."):]       # æˆ–è€…ç”¨ k.replace("enc.", "", 1)
        #     else:
        #         stripped = k
        #     # 2) æ‹¼ä¸Šä½ çš„ prefix
        #     new_k = prefix + stripped

        #     # 3) çœ‹çœ‹æ‹¼å¥½çš„ key åœ¨ä¸åœ¨ model é‡Œï¼Œåœ¨çš„è¯å°±åŠ è½½
        #     if new_k in model_keys:
        #         fixed_sd[new_k] = v
        #     # else: è‡ªåŠ¨è·³è¿‡

        # missing, unexpected = self.featurizer.load_state_dict(fixed_sd, strict=False)
        # print(f"âœ… Actually loaded {len(fixed_sd)} weights")
        # print("â— Missing in loaded (model wants but you didn't give):", missing)
        # print("â— Unexpected in state (you gave but model doesn't need):", unexpected)
        
        
                # å†»ç»“ featurizer
        # for param in self.featurizer.parameters():
        #     param.requires_grad = False
        # print("âœ… Featurizer parameters have been frozen.")

        # self.optimizer = torch.optim.Adam(
        #     filter(lambda p: p.requires_grad, self.parameters()),
        #     lr=self.hparams['lr']
        # )



            
    def predict(self, x, y, return_context = False, past_key_values = None): 
        # print("å˜»å˜»")
        if x.ndim == 4:                                                             # Splits a batch into multiple sequences with length as the context length                                 
            # bs, c, h, w = x.size()                          
            # bs, ctxt = bs // self.train_context_len, self.train_context_len#å°†æ‰¹æ¬¡å¤§å° bs åˆ†å‰²æˆå¤šä¸ªä¸Šä¸‹æ–‡å—ï¼ˆctxtï¼‰ï¼Œself.context_len æ˜¯æ¯ä¸ªä¸Šä¸‹æ–‡å—çš„é•¿åº¦ã€‚æœ€ç»ˆ bs è¢«é‡æ–°è®¡ç®—ä¸ºæŒ‰ä¸Šä¸‹æ–‡é•¿åº¦åˆ’åˆ†çš„æ‰¹æ¬¡æ•°é‡ã€‚
            # y = y.reshape(bs, ctxt)#å°†æ ‡ç­¾ y é‡å¡‘ä¸ºé€‚åº”æ–°çš„æ‰¹æ¬¡å’Œä¸Šä¸‹æ–‡é•¿åº¦
            B, C, H, W = x.size()
            # 2) ç¡®å®šè®­ç»ƒæ—¶çš„ä¸Šä¸‹æ–‡é•¿åº¦
            ctxt = self.train_context_len   # æ¯”å¦‚ 75

            # â€”â€” åœ¨è¿™é‡Œæ’å…¥è£å‰ª full çš„é€»è¾‘ â€”â€” 
            full = (B // ctxt) * ctxt         # e.g. (100//75)*75 = 75
            x_cut = x[:full]              # ä¿ç•™å‰ 75 å¼ å›¾
            y_cut = y[:full]              # ä¿ç•™å‰ 75 ä¸ªæ ‡ç­¾

            bs = full // ctxt               # 75//75 = 1

            # 3) åˆ‡åˆ†æˆ [bs, cl, C, H, W]
            x = x_cut.view(bs, ctxt, C, H, W)
            y = y_cut.view(bs, ctxt)

            # 4) åˆå¹¶å› featurizer è¦çš„ 4D è¾“å…¥
            x = x.view(bs * ctxt, C, H, W)
        elif x.ndim == 5:   
            bs, ctxt, c, h, w = x.size()
            x = x.contiguous().view(bs * ctxt, c, h, w)
        else:
            raise NotImplementedError
        
        x = self.featurizer(x)
        # print("å¯¹å¯¹å¯¹åˆ°è¿™é‡Œå•¦ï¼ï¼ï¼")                                               
        x = x.reshape(bs, ctxt, -1)    #å°†ç‰¹å¾æ•°æ® x é‡å¡‘ä¸ºæ–°çš„å½¢çŠ¶ã€‚bs æ˜¯æ‰¹æ¬¡å¤§å°ï¼Œctxt æ˜¯ä¸Šä¸‹æ–‡é•¿åº¦                                      
        outputs = self.classifier((x, y, None, past_key_values))  #è®°å½•äº†å‰é¢å·²ç»å¤„ç†çš„æ•°æ®å¯¹å½“å‰é¢„æµ‹çš„å½±å“
        p, past = outputs[0], outputs[1] #pçš„å½¢çŠ¶æ˜¯ [batch_size, context_len, n_outputs]ï¼Œè¡¨ç¤ºæ¯ä¸ªè¾“å…¥çš„é¢„æµ‹ç»“æœ ã€‚paståŒ…å« GPT-2 æ¨¡å‹ç”Ÿæˆçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚å®ƒé€šå¸¸ç”¨äºä¿å­˜æ¯æ¬¡å‰å‘ä¼ æ’­è®¡ç®—çš„ä¸­é—´å€¼ï¼Œä»¥ä¾¿åœ¨åç»­çš„ç”Ÿæˆä»»åŠ¡ä¸­åŠ é€Ÿæ¨ç†ã€‚
        if return_context:  return p, past
        else:   return p.view(-1, p.size(-1))
           
    
    def repeat_past_key_values(self, past_key_values, repeats):                     # process key value cache for computing fast inference
        repeated_past_key_values = []
        for layer_past in past_key_values:
            repeated_layer_past = []
            for tensor in layer_past:
                if tensor is not None:
                    repeated_tensor = tensor.repeat_interleave(repeats=repeats, dim=0)
                else:
                    repeated_tensor = None
                repeated_layer_past.append(repeated_tensor)
            repeated_past_key_values.append(tuple(repeated_layer_past))
        return tuple(repeated_past_key_values)



    # def repeat_past_key_values(self, past_key_values, repeats):
    #     if past_key_values is None:
    #         return None
            
    #     # æ£€æŸ¥æ˜¯å¦æ˜¯MosaicTransformerçš„ç¼“å­˜æ ¼å¼ï¼ˆå­—å…¸ï¼‰
    #     if isinstance(past_key_values, dict):
    #         # å¤„ç†MosaicTransformerçš„ç¼“å­˜æ ¼å¼
    #         repeated_past = {}
    #         for key, value in past_key_values.items():
    #             if isinstance(value, torch.Tensor):
    #                 repeated_past[key] = value.repeat_interleave(repeats=repeats, dim=0)
    #             else:
    #                 repeated_past[key] = value
    #         return repeated_past
        
    #     # åŸæ¥çš„GPT2æ ¼å¼å¤„ç†é€»è¾‘
    #     repeated_past_key_values = []
    #     for layer_past in past_key_values:
    #         repeated_layer_past = []
    #         for tensor in layer_past:
    #             if tensor is not None:
    #                 repeated_tensor = tensor.repeat_interleave(repeats=repeats, dim=0)
    #             else:
    #                 repeated_tensor = None
    #             repeated_layer_past.append(repeated_tensor)
    #         repeated_past_key_values.append(tuple(repeated_layer_past))
    #     return tuple(repeated_past_key_values)
    
    
    # def repeat_past_key_values(self, past_key_values, repeats):
    #     if past_key_values is None or repeats == 1:
    #         return past_key_values

    #     # Mosaic: dict
    #     if isinstance(past_key_values, dict):
    #         rep = {}
    #         for k, v in past_key_values.items():
    #             if torch.is_tensor(v):
    #                 rep[k] = v.repeat_interleave(repeats, dim=0)
    #             else:
    #                 rep[k] = v  # æ ‡é‡/é•¿åº¦ç­‰ä¿¡æ¯åŸæ ·è¿”å›
    #         return rep

    #     # GPT-2: tuple of tuples
    #     rep_layers = []
    #     for layer_past in past_key_values:
    #         rep_layer = []
    #         for t in layer_past:
    #             rep_layer.append(t.repeat_interleave(repeats, dim=0) if t is not None else None)
    #         rep_layers.append(tuple(rep_layer))
    #     return tuple(rep_layers)
    
    
    # def repeat_past_key_values(self,past_key_values, repeats: int):
    #     """
    #     Repeat cached KV along batch dim (dim=0).
    #     æ”¯æŒï¼š
    #     - None
    #     - list/tuple of (k,v) pairs: [(k,v), ...]  (Memory Mosaic / GPT2 å‡å¯)
    #     - tuple of tuples (GPT-2 åŸç‰ˆ)
    #     - dict é‡Œå­˜ tensor çš„æƒ…å†µï¼ˆæ—§çš„ cached_embeds å®ç°ï¼‰
    #     """
    #     if past_key_values is None or repeats == 1:
    #         return past_key_values

    #     # dict æƒ…å†µ
    #     if isinstance(past_key_values, dict):
    #         rep = {}
    #         for k, v in past_key_values.items():
    #             if torch.is_tensor(v):
    #                 rep[k] = v.repeat_interleave(repeats, dim=0)
    #             else:
    #                 rep[k] = v
    #         return rep

    #     # list / tuple æƒ…å†µ
    #     is_list = isinstance(past_key_values, list)
    #     out_layers = []
    #     for layer_past in past_key_values:
    #         if layer_past is None:
    #             out_layers.append(None)
    #             continue
    #         rep_tensors = []
    #         for t in layer_past:
    #             if t is None:
    #                 rep_tensors.append(None)
    #             else:
    #                 rep_tensors.append(t.repeat_interleave(repeats, dim=0))
    #         out_layers.append(tuple(rep_tensors))

    #     return out_layers if is_list else tuple(out_layers)

    
    @torch.no_grad()
    def _evaluate_robust(self, model, loader, metrics=['accuracy'], test_cache=None):
        """
        ICRM é£æ ¼é²æ£’è¯„ä¼°ï¼šå¯¹ test_ctxt ä¸­æ¯ä¸ª context_val
        1) ç”¨ test_cache çš„å‰ context_val æ­¥åš teacher-forcing é¢„çƒ­ â†’ initial_past
        2) å¯¹è¯„ä¼° loaderï¼ˆå•æ­¥æˆ–å¤šæ­¥ï¼‰ç»§ç»­åš teacher-forcing æ¨ç†ï¼Œè®¡ç®—æŒ‡æ ‡
        """
        assert test_cache is not None and len(test_cache) == 2
        test_cache_x, test_cache_y = test_cache

        self.network.eval()
        model.eval()

        device = self.device
        result = {}

        # ä½ç½®ä¸Šé™ä¿æŠ¤ï¼ˆä¸Šä¸‹æ–‡ 2*context_val tokenï¼‰
        n_pos = getattr(self.classifier._backbone.config, 'n_positions', 1024)

        for context_val in self.test_ctxt:
            with torch.no_grad():
                # ---------- (1) é¢„çƒ­ï¼šç”¨ç¼“å­˜åºåˆ—çš„å‰ context_val æ­¥æ„é€  initial_past ----------
                if context_val == 0:
                    initial_past = None
                    tokens_used = 0
                else:
                    # ---------- (1) é¢„çƒ­ï¼šæ”¹ä¸ºâ€œè‡ªå›å½’â€å¾—åˆ° initial_past ----------
                    # å–å‰ context_val æ­¥ï¼Œå¹¶æ¬åˆ° deviceï¼›è¿™é‡Œåªç”¨ x0 åšè‡ªå›å½’ï¼Œy0 åªä¸ºå¯¹é½å½¢çŠ¶/è®¾å¤‡ï¼ˆä¸ç”¨çœŸå€¼ï¼‰
                    x0 = test_cache_x[:, :context_val].to(device)          # æœŸæœ› [B0, context_val, 3, H, W] æˆ– [B0, 3, H, W]
                    if x0.ndim == 4:                                       # è‹¥æ˜¯ [B0,3,H,W]ï¼Œè¡¥æ—¶é—´ç»´
                        x0 = x0.unsqueeze(1)                                # -> [B0,1,3,H,W]
                    B0, T0 = x0.size(0), x0.size(1)

                    # ä½ç½®ä¸Šé™æ£€æŸ¥ï¼šAR é¢„çƒ­æ¯æ­¥ä¹Ÿä¼šè¿½åŠ  2 ä¸ª tokenï¼ˆx_t ä¸ y_tï¼‰
                    assert 2 * T0 <= n_pos, \
                        f"warmup length(2*{T0}) exceeds n_positions({n_pos})"

                    # AR é¢„çƒ­ï¼šä» BOS å¼€å§‹ï¼Œä¸€æ­¥æ­¥é¢„æµ‹ yÌ‚_t å¹¶ç´¯ç§¯ past
                    bos_id = self.classifier.label_embed.num_embeddings - 1   # çº¦å®š BOS çš„ idï¼ˆç¡®ä¿ embedding é‡Œé¢„ç•™äº†è¿™ä¸ªæ§½ä½ï¼‰
                    prev_y = torch.full((B0, 1), bos_id, dtype=torch.long, device=device)  # ç¬¬ä¸€æ­¥çš„â€œä¸Šä¸€æ ‡ç­¾â€
                    past = None

                    for t in range(T0):
                        x_step = x0[:, t].unsqueeze(1)                        # [B0,1,3,H,W]
                        # å…³é”®ï¼šä¸€æ­¥è‡ªå›å½’ï¼›å†…éƒ¨ä¼šæ„äº¤æ›¿åºåˆ—å¹¶æŠŠæœ¬æ­¥ä¸¤ä¸ª token ç´¯åˆ° past é‡Œ
                        logits_1, past = self.predict(
                            x_step,                      # [B0,1,3,H,W]
                            prev_y,                      # [B0,1]ï¼ˆt=0 ç”¨ BOSï¼Œå…¶åç”¨ä¸Šä¸€æ—¶åˆ»é¢„æµ‹ï¼‰
                            return_context=True,
                            past_key_values=past
                        )                                # logits_1:[B0,1,C]ï¼Œpast æ›´æ–°

                        # ä¸‹ä¸€æ­¥çš„ä¸Šä¸€æ ‡ç­¾ = æœ¬æ­¥é¢„æµ‹
                        prev_y = logits_1[:, 0, :].argmax(dim=-1, keepdim=True)  # [B0,1]

                    # é¢„çƒ­å®Œæˆï¼Œä¿å­˜ pastï¼Œå¹¶è®°å½• token å ç”¨ï¼ˆæ¯æ­¥ +2ï¼‰
                    initial_past = past
                    tokens_used  = 2 * T0
                    
                # ---------- (2) æ­£å¼è¯„ä¼°ï¼šå¯¹ loader åš teacher-forcing ----------
                all_logits = []
                all_labels = []

                for x, y in loader:
                    # é€è®¾å¤‡ & è§„èŒƒå½¢çŠ¶åˆ° [B,T,3,H,W] / [B,T]
                    x = x.to(device)
                    y = y.to(device).long()

                    if x.ndim == 4:                 # [B,3,H,W] -> [B,1,3,H,W]
                        x = x.unsqueeze(1)
                    if y.ndim == 1:                 # [B] -> [B,1]
                        y = y.unsqueeze(1)

                    B, T = x.size(0), x.size(1)

                    # æŠŠ initial_past é‡å¤åˆ°å½“å‰ batch å¤§å°ï¼ˆè‹¥éœ€è¦ï¼‰
                    past = None
                    if initial_past is not None:
                        # ä½ åŸæ¥æœ‰ repeat_past_key_valuesï¼›è¿™é‡ŒæŒ‰å½“å‰ B é‡å¤å³å¯
                        past = self.repeat_past_key_values(initial_past, B)

                    # ä½ç½®ä¸Šé™æ£€æŸ¥ï¼šé¢„çƒ­ tokens + æœ¬æ‰¹å°†è¿½åŠ çš„ 2*T
                    if tokens_used + 2 * T > n_pos:
                        # è¶…é™åˆ™ä¸¢å¼ƒ warmupï¼Œç›´æ¥ç”¨æœ¬æ‰¹è‡ªå·±åš teacher-forcingï¼ˆä¸åŸå§‹å®ç°ä¸€è‡´ï¼‰
                        past = None

                    # teacher-forcing æ¨ç†ï¼špredict è¿”å› [B,T,C]ï¼ˆåªå–â€œxä½â€çš„ logitsï¼‰
                    p, _ = self.predict(x, y, return_context=True, past_key_values=past)  # p:[B,T,C]

                    # å±•å¹³æˆ [B*T, C] / [B*T] ä»¥ä¾¿ç»Ÿä¸€è®¡ç®—æŒ‡æ ‡
                    all_logits.append(p.reshape(-1, p.size(-1)))
                    all_labels.append(y.reshape(-1))

                if len(all_logits) == 0:
                    # ç©º loader çš„å…œåº•
                    metric_results = {m: 0.0 for m in metrics}
                else:
                    ys_pred = torch.cat(all_logits, dim=0)  # [Î£(B*T), C]
                    ys_true = torch.cat(all_labels, dim=0)  # [Î£(B*T)]
                    metric_results = utils.compute_metric(metrics, ys_pred, ys_true)

                # ä¸åŸç‰ˆ key ä¸€è‡´ï¼še-(self.context_len - context_val)
                remain = max(0, getattr(self, 'context_len', 0) - context_val)
                for m in metrics:
                    result[f'{m}(e-{remain})'] = metric_results[m]

        self.network.train()
        model.train()
        return result

    
    def evaluate(self, loader, module = 'train', cache = None):
        self.test_ctxt = list(range(0, 51, 5)) if module == 'test' else [0, 25, 50, 75, 100]   #å®šä¹‰ä¸Šä¸‹æ–‡çš„é•¿åº¦
        result = self._evaluate_robust(self.network, loader, self.hparams['metrics'],  cache)
        return result
 

class ARM_CML(ERM):
    """ Adaptive Risk Minimization (ARM) - (Context Model)"""
    def __init__(self, input_shape, num_classes, hparams):
        original_input_shape = input_shape
        self.n_context_channels = hparams.get('num_features', 1)
        self.ctxt = hparams['support_size']
        self.orig_ctxt = hparams['support_size']
        self.adapt_bn = hparams['adapt_bn']
        input_shape =  (self.n_context_channels + input_shape[0],) + input_shape[1:]             # Since we concatenate the context with input x
        super(ARM_CML, self).__init__(input_shape, num_classes, hparams)
        if hasattr(networks, hparams['context_net']):                                                
            self.context_net = getattr(networks, hparams['context_net'])(original_input_shape[0], hparams)  
            if hparams['is_parallel']:
                self.context_net = utils.data_parallel(self.context_net)
        else:
            raise NotImplementedError()
        
        #  Joint optimizer for Ï• and ğœƒ
        params = list(self.network.parameters()) + list(self.context_net.parameters())
        self.optimizer = utils.extract_optimizer(self.hparams['optimizer_name'], params, lr = self.hparams['lr'], weight_decay = self.hparams['weight_decay'])
        self.hparams['mode'] = 'train'
        
        
    def predict(self, x, y = None, model = None):
        bs, _, h, w = x.shape
        re = bs  % self.ctxt
        if self.hparams['mode'] == 'train':
            assert re == 0, 'During training, makre sure batch size is a multiple of support'
            
        if self.hparams['mode'] == 'test' and re != 0:
            x = torch.cat([x, x[:(self.ctxt - re)].clone()], dim=0) 
            y = torch.cat([y, y[:(self.ctxt - re)].clone()], dim=0) 

        eff_bs, supp_size = len(x) // self.ctxt, self.ctxt
        ctxt = self.context_net(x)
        ctxt = ctxt.reshape(eff_bs, supp_size, self.n_context_channels, h, w).mean(dim=1)    
        ctxt = torch.repeat_interleave(ctxt, repeats = supp_size, dim=0)
        x = torch.cat([x, ctxt], dim=1)
        if self.hparams['mode'] == 'test':
            return self.network(x), y
        return self.network(x)
 
    def _evaluate(self, model, loader, metrics=['accuracy']):
        metrics = metrics or self.metrics
        model.eval()
        self.context_net.eval()
        result = {key: 0 for key in metrics}
        total = 0
        with torch.no_grad():
            for x, y in loader:
                x, y = x.to(self.device), y.to(self.device)
                p, y_modif = self.predict(x, y, model=model)
                batch_results = utils.compute_metric(metrics, p, y_modif)
                for metric in metrics:
                    result[metric] += batch_results[metric] * len(y_modif)
                total += len(y_modif)
            for metric in metrics:  result[metric] /= (total + 1e-9)
            model.train()
            return result
   
    def evaluate(self, loader, module = 'train', cache = None):
        self.hparams['mode'] = 'test'
        self.hparams['test_support'] = self.hparams['test_support'] if self.hparams['test_support'] is not None else self.ctxt         
        result = {}
        self.test_ctxt = range(0, 51, 5) if module == 'test' else [0, 25, 50, 75, 100]
        for supp in self.test_ctxt:  
            if supp == 0:
                self.ctxt = supp + 1
            else:
                self.ctxt = supp
            metric_results = self._evaluate(self.network, loader, self.hparams['metrics'])
            result.update({f'{metric}(e-{self.hparams["test_support"] - supp})': metric_results[metric] for metric in self.hparams['metrics']})    
        self.hparams['mode'] = 'train'
        self.context_net.train()
        self.network.train()
        self.ctxt = self.orig_ctxt
        return result 

 

class Mixup(ERM):
    """
    Mixup of minibatches from different domains
    https://arxiv.org/pdf/2001.00677.pdf
    https://arxiv.org/pdf/1912.01805.pdf
    """
    def __init__(self, input_shape, num_classes, hparams):
        super(Mixup, self).__init__(input_shape, num_classes,
                                    hparams)

    def update(self, minibatches, unlabeled=None):
        self.network.train()
        objective = 0
        for (xi, yi), (xj, yj) in utils.random_pairs_of_minibatches(minibatches):
            lam = np.random.beta(self.hparams["mixup_alpha"], self.hparams["mixup_alpha"])

            x = lam * xi + (1 - lam) * xj
            predictions = self.predict(x)
            objective += lam * F.cross_entropy(predictions, yi)
            objective += (1 - lam) * F.cross_entropy(predictions, yj)

        objective /= len(minibatches)

        self.optimizer.zero_grad()
        objective.backward()
        self.optimizer.step()

        return {'train_loss': objective.item()}
    

class IB_ERM(ERM):
    """Information Bottleneck based ERM on feature with conditionning"""

    def __init__(self, input_shape, num_classes, hparams):
        super(IB_ERM, self).__init__(input_shape, num_classes, hparams)
        self.optimizer = utils.extract_optimizer(self.hparams['optimizer_name'], list(self.featurizer.parameters()) + list(self.classifier.parameters()), lr=self.hparams["lr"], weight_decay=self.hparams['weight_decay'])
        self.register_buffer('update_count', torch.tensor([0]))
        
    @property
    def ib_penalty_weight(self):
        return self.hparams['ib_lambda'] if self.update_count >= self.hparams['ib_penalty_anneal_iters'] else 0.0

    def update(self, minibatches, unlabeled=None):
        self.network.train()

        all_x = torch.cat([x for x, y in minibatches])
        all_features = self.featurizer(all_x)
        all_logits = self.classifier(all_features)
        features_list = torch.split(all_features, [x.shape[0] for x, y in minibatches])
        logits_list = torch.split(all_logits, [x.shape[0] for x, y in minibatches])
        
        nll = torch.mean(torch.stack([F.cross_entropy(logits, y) for logits, (x, y) in zip(logits_list, minibatches)]))
        ib_penalty = torch.mean(torch.stack([features.var(dim=0).mean() for features in features_list]))
        loss = nll + self.ib_penalty_weight * ib_penalty
        
        if self.update_count == self.hparams['ib_penalty_anneal_iters']:
            self.optimizer = utils.extract_optimizer(self.hparams['optimizer_name'], list(self.featurizer.parameters()) + list(self.classifier.parameters()), lr=self.hparams["lr"], weight_decay=self.hparams['weight_decay'])

        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        self.update_count += 1
        return {'train_loss': loss.item(), 'nll': nll.item(), 'IB_penalty': ib_penalty.item()}
       
              
class IB_IRM(ERM):
    """Information Bottleneck based IRM on feature with conditionning"""

    def __init__(self, input_shape, num_classes, hparams):
        super(IB_IRM, self).__init__(input_shape, num_classes,
                                  hparams)
        self.optimizer = utils.extract_optimizer(self.hparams['optimizer_name'], list(self.featurizer.parameters()) + list(self.classifier.parameters()), lr=self.hparams["lr"], weight_decay=self.hparams['weight_decay'])
        self.register_buffer('update_count', torch.tensor([0]))

    @staticmethod
    def _irm_penalty(logits, y):
        device = "cuda" if logits[0][0].is_cuda else "cpu"
        scale = torch.tensor(1., device=device, requires_grad=True)
        loss_1 = F.cross_entropy(logits[::2] * scale, y[::2])
        loss_2 = F.cross_entropy(logits[1::2] * scale, y[1::2])
        grad_1 = autograd.grad(loss_1, [scale], create_graph=True)[0]
        grad_2 = autograd.grad(loss_2, [scale], create_graph=True)[0]
        return torch.sum(grad_1 * grad_2)

    @property
    def irm_penalty_weight(self):
        return self.hparams['irm_lambda'] if self.update_count >= self.hparams['irm_penalty_anneal_iters'] else 1.0

    @property
    def ib_penalty_weight(self):
        return self.hparams['ib_lambda'] if self.update_count >= self.hparams['ib_penalty_anneal_iters'] else 0.0

    def update(self, minibatches, unlabeled=None):
        self.network.train()
        all_x = torch.cat([x for x, y in minibatches])
        all_features = self.featurizer(all_x)
        all_logits = self.classifier(all_features)
        features_list = torch.split(all_features, [x.shape[0] for x, y in minibatches])
        logits_list = torch.split(all_logits, [x.shape[0] for x, y in minibatches])

        nll = torch.mean(torch.stack([F.cross_entropy(logits, y) for logits, (x, y) in zip(logits_list, minibatches)]))
        irm_penalty = torch.mean(torch.stack([self._irm_penalty(logits, y) for logits, (x, y) in zip(logits_list, minibatches)]))
        ib_penalty = torch.mean(torch.stack([features.var(dim=0).mean() for features in features_list]))

        loss = nll + self.irm_penalty_weight * irm_penalty + self.ib_penalty_weight * ib_penalty

        if self.update_count == self.hparams['irm_penalty_anneal_iters'] or self.update_count == self.hparams['ib_penalty_anneal_iters']:
            self.optimizer = utils.extract_optimizer(self.hparams['optimizer_name'], list(self.featurizer.parameters()) + list(self.classifier.parameters()), lr=self.hparams["lr"], weight_decay=self.hparams['weight_decay'])

        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        self.update_count += 1
        return {'train_loss': loss.item(), 'nll': nll.item(), 'IRM_penalty': irm_penalty.item(), 'IB_penalty': ib_penalty.item()}
    

class Fish(ERM):
    """
    Implementation of Fish, as seen in Gradient Matching for Domain
    Generalization, Shi et al. 2021.
    """

    def __init__(self, input_shape, num_classes, hparams):
        super(Fish, self).__init__(input_shape, num_classes,
                                   hparams)
        self.input_shape = input_shape
        self.num_classes = num_classes

        self.network = networks.WholeFish(input_shape, num_classes, hparams)
        self.optimizer = utils.extract_optimizer(self.hparams['optimizer_name'], self.network.parameters(), lr=self.hparams["lr"],weight_decay=self.hparams['weight_decay'])
        self.optimizer_inner_state = None

    def create_clone(self, device):
        self.network_inner = networks.WholeFish(self.input_shape, self.num_classes, self.hparams, weights=self.network.state_dict()).to(device)
        self.optimizer_inner = utils.extract_optimizer(self.hparams['optimizer_name'],self.network_inner.parameters(),lr=self.hparams["lr"],weight_decay=self.hparams['weight_decay'])
        if self.optimizer_inner_state is not None:
            self.optimizer_inner.load_state_dict(self.optimizer_inner_state)

    def fish(self, meta_weights, inner_weights, lr_meta):
        meta_weights = utils.ParamDict(meta_weights)
        inner_weights = utils.ParamDict(inner_weights)
        meta_weights += lr_meta * (inner_weights - meta_weights)
        return meta_weights

    def update(self, minibatches, unlabeled=None):
        self.network.train()
        self.create_clone(minibatches[0][0].device)
        for x, y in minibatches:
            loss = F.cross_entropy(self.network_inner(x), y)
            self.optimizer_inner.zero_grad()
            loss.backward()
            self.optimizer_inner.step()

        self.optimizer_inner_state = self.optimizer_inner.state_dict()
        meta_weights = self.fish(meta_weights=self.network.state_dict(),inner_weights=self.network_inner.state_dict(),lr_meta=self.hparams["meta_lr"])
        self.network.reset_weights(meta_weights)

        return {'train_loss': loss.item()}